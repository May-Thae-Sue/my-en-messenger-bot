starting at နွေ ဩ 4 23:30:10 +0630 2019 on may-thae-sue
/home/thaesue/smt_se/tools/snt2cooc.out /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en.vcb /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my.vcb /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my-en-int-train.snt > /home/thaesue/smt_se/Data/model8/my-en/training/giza-inverse.1/my-en.cooc
/home/thaesue/smt_se/tools/GIZA++  -CoocurrenceFile /home/thaesue/smt_se/Data/model8/my-en/training/giza-inverse.1/my-en.cooc -c /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/thaesue/smt_se/Data/model8/my-en/training/giza-inverse.1/my-en -onlyaldumps 1 -p0 0.999 -s /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en.vcb -t /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my.vcb
Parameter 'coocurrencefile' changed from '' to '/home/thaesue/smt_se/Data/model8/my-en/training/giza-inverse.1/my-en.cooc'
Parameter 'c' changed from '' to '/home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my-en-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-08-04.233011.thaesue' to '/home/thaesue/smt_se/Data/model8/my-en/training/giza-inverse.1/my-en'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en.vcb'
Parameter 't' changed from '' to '/home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-08-04.233011.thaesue.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/thaesue/smt_se/Data/model8/my-en/training/giza-inverse.1/my-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en.vcb  (source vocabulary file name)
t = /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-08-04.233011.thaesue.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/thaesue/smt_se/Data/model8/my-en/training/giza-inverse.1/my-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en.vcb  (source vocabulary file name)
t = /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Source vocabulary list has 4306 unique tokens 
Target vocabulary list has 6095 unique tokens 
Calculating vocabulary frequencies from corpus /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my-en-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 11896 sentence pairs.
 Train total # sentence pairs (weighted): 11896
Size of source portion of the training corpus: 59653 tokens
Size of the target portion of the training corpus: 69961 tokens 
In source portion of the training corpus, only 4305 unique tokens appeared
In target portion of the training corpus, only 6093 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 69961/(71549-11896)== 1.1728
There are 116782 116782 entries in table
==========================================================
Model1 Training Started at: Sun Aug  4 23:30:11 2019

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 13.1009 PERPLEXITY 8785.59
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 15.8653 PERPLEXITY 59695.4
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 5.52634 PERPLEXITY 46.0888
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 6.93125 PERPLEXITY 122.044
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 5.00434 PERPLEXITY 32.0963
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 6.14086 PERPLEXITY 70.564
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 4.8394 PERPLEXITY 28.6289
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.76729 PERPLEXITY 54.4661
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 4.76964 PERPLEXITY 27.2775
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.56656 PERPLEXITY 47.3917
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 0 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 4305  #classes: 51
Read classes: #words: 6094  #classes: 51

==========================================================
Hmm Training Started at: Sun Aug  4 23:30:11 2019

-----------
Hmm: Iteration 1
A/D table contains 15955 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.73388 PERPLEXITY 26.6096
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 5.44509 PERPLEXITY 43.5647

Hmm Iteration: 1 took: 1 seconds

-----------
Hmm: Iteration 2
A/D table contains 15955 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 4.37308 PERPLEXITY 20.7218
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 4.83007 PERPLEXITY 28.4444

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 15955 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 4.13536 PERPLEXITY 17.5739
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 4.47067 PERPLEXITY 22.1721

Hmm Iteration: 3 took: 1 seconds

-----------
Hmm: Iteration 4
A/D table contains 15955 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 3.96608 PERPLEXITY 15.6282
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 4.23171 PERPLEXITY 18.7876

Hmm Iteration: 4 took: 0 seconds

-----------
Hmm: Iteration 5
A/D table contains 15955 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 3.84687 PERPLEXITY 14.3887
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 4.05945 PERPLEXITY 16.6731

Hmm Iteration: 5 took: 0 seconds

Entire Hmm Training took: 2 seconds
==========================================================
Read classes: #words: 4305  #classes: 51
Read classes: #words: 6094  #classes: 51
Read classes: #words: 4305  #classes: 51
Read classes: #words: 6094  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sun Aug  4 23:30:14 2019


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 55.7061 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 15955 parameters.
A/D table contains 12691 parameters.
p0_count is 57932.1 and p1 is 6011.47; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 3.29159 PERPLEXITY 9.7919
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 3.43844 PERPLEXITY 10.8411

THTo3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 55.5759 #alsophisticatedcountcollection: 0 #hcsteps: 1.56019
#peggingImprovements: 0
A/D table contains 15955 parameters.
A/D table contains 12747 parameters.
p0_count is 66268 and p1 is 1846.5; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.91363 PERPLEXITY 30.1405
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 5.01868 PERPLEXITY 32.417

Model3 Viterbi Iteration : 2 took: 0 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 55.544 #alsophisticatedcountcollection: 0 #hcsteps: 1.65678
#peggingImprovements: 0
A/D table contains 15955 parameters.
A/D table contains 12778 parameters.
p0_count is 68244.3 and p1 is 858.359; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.74595 PERPLEXITY 26.8332
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 4.8272 PERPLEXITY 28.3879

Model3 Viterbi Iteration : 3 took: 1 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 55.5653 #alsophisticatedcountcollection: 7.48462 #hcsteps: 1.58373
#peggingImprovements: 0
D4 table contains 418383 parameters.
A/D table contains 15955 parameters.
A/D table contains 12640 parameters.
p0_count is 68571 and p1 is 695.01; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 4.68741 PERPLEXITY 25.7663
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 4.75669 PERPLEXITY 27.0337

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 55.5498 #alsophisticatedcountcollection: 6.12878 #hcsteps: 1.50084
#peggingImprovements: 0
D4 table contains 418383 parameters.
A/D table contains 15955 parameters.
A/D table contains 13259 parameters.
p0_count is 68580.6 and p1 is 690.197; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 4.39669 PERPLEXITY 21.0637
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 4.4471 PERPLEXITY 21.8127

Model4 Viterbi Iteration : 5 took: 2 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 55.5413 #alsophisticatedcountcollection: 5.28791 #hcsteps: 1.44713
#peggingImprovements: 0
D4 table contains 418383 parameters.
A/D table contains 15955 parameters.
A/D table contains 13078 parameters.
p0_count is 68636.4 and p1 is 662.315; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 4.302 PERPLEXITY 19.7256
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 4.34184 PERPLEXITY 20.2779

Model4 Viterbi Iteration : 6 took: 1 seconds
H333444 Training Finished at: Sun Aug  4 23:30:18 2019


Entire Viterbi H333444 Training took: 4 seconds
==========================================================

Entire Training took: 7 seconds
Program Finished at: Sun Aug  4 23:30:18 2019

==========================================================
finished at နွေ ဩ 4 23:30:18 +0630 2019
