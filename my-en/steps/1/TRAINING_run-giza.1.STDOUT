starting at နွေ ဩ 4 23:30:15 +0630 2019 on may-thae-sue
/home/thaesue/smt_se/tools/snt2cooc.out /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my.vcb /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en.vcb /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en-my-int-train.snt > /home/thaesue/smt_se/Data/model8/my-en/training/giza.1/en-my.cooc
/home/thaesue/smt_se/tools/GIZA++  -CoocurrenceFile /home/thaesue/smt_se/Data/model8/my-en/training/giza.1/en-my.cooc -c /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en-my-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/thaesue/smt_se/Data/model8/my-en/training/giza.1/en-my -onlyaldumps 1 -p0 0.999 -s /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my.vcb -t /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en.vcb
Parameter 'coocurrencefile' changed from '' to '/home/thaesue/smt_se/Data/model8/my-en/training/giza.1/en-my.cooc'
Parameter 'c' changed from '' to '/home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en-my-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '119-08-04.233016.thaesue' to '/home/thaesue/smt_se/Data/model8/my-en/training/giza.1/en-my'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my.vcb'
Parameter 't' changed from '' to '/home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-08-04.233016.thaesue.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/thaesue/smt_se/Data/model8/my-en/training/giza.1/en-my  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en-my-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my.vcb  (source vocabulary file name)
t = /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 119-08-04.233016.thaesue.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/thaesue/smt_se/Data/model8/my-en/training/giza.1/en-my  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en-my-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/my.vcb  (source vocabulary file name)
t = /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Source vocabulary list has 6095 unique tokens 
Target vocabulary list has 4306 unique tokens 
Calculating vocabulary frequencies from corpus /home/thaesue/smt_se/Data/model8/my-en/training/prepared.1/en-my-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 11896 sentence pairs.
 Train total # sentence pairs (weighted): 11896
Size of source portion of the training corpus: 69961 tokens
Size of the target portion of the training corpus: 59653 tokens 
In source portion of the training corpus, only 6094 unique tokens appeared
In target portion of the training corpus, only 4304 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 59653/(81857-11896)== 0.852661
There are 114993 114993 entries in table
==========================================================
Model1 Training Started at: Sun Aug  4 23:30:16 2019

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 12.7157 PERPLEXITY 6726.9
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 15.6402 PERPLEXITY 51071.7
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 4.77444 PERPLEXITY 27.3684
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 6.28738 PERPLEXITY 78.1067
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 4.23692 PERPLEXITY 18.8556
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.40641 PERPLEXITY 42.4123
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 4.06046 PERPLEXITY 16.6848
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.01407 PERPLEXITY 32.3136
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 3.98797 PERPLEXITY 15.8671
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 4.81631 PERPLEXITY 28.1742
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 0 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 6094  #classes: 51
Read classes: #words: 4305  #classes: 51

==========================================================
Hmm Training Started at: Sun Aug  4 23:30:16 2019

-----------
Hmm: Iteration 1
A/D table contains 14601 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 3.95198 PERPLEXITY 15.4762
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 4.70156 PERPLEXITY 26.0202

Hmm Iteration: 1 took: 1 seconds

-----------
Hmm: Iteration 2
A/D table contains 14601 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 3.57199 PERPLEXITY 11.8926
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 4.07603 PERPLEXITY 16.8658

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 14601 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 3.35821 PERPLEXITY 10.2547
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 3.75732 PERPLEXITY 13.5228

Hmm Iteration: 3 took: 0 seconds

-----------
Hmm: Iteration 4
A/D table contains 14601 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 3.2344 PERPLEXITY 9.41131
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 3.57147 PERPLEXITY 11.8883

Hmm Iteration: 4 took: 1 seconds

-----------
Hmm: Iteration 5
A/D table contains 14601 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 3.16138 PERPLEXITY 8.94685
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 3.45183 PERPLEXITY 10.9422

Hmm Iteration: 5 took: 0 seconds

Entire Hmm Training took: 2 seconds
==========================================================
Read classes: #words: 6094  #classes: 51
Read classes: #words: 4305  #classes: 51
Read classes: #words: 6094  #classes: 51
Read classes: #words: 4305  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sun Aug  4 23:30:18 2019


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 51.5484 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 14601 parameters.
A/D table contains 15352 parameters.
p0_count is 52583.4 and p1 is 3534.69; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 2.54691 PERPLEXITY 5.8438
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 2.7332 PERPLEXITY 6.64929

THTo3 Viterbi Iteration : 1 took: 1 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 51.5245 #alsophisticatedcountcollection: 0 #hcsteps: 1.56742
#peggingImprovements: 0
A/D table contains 14601 parameters.
A/D table contains 15324 parameters.
p0_count is 57853.4 and p1 is 899.81; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.69681 PERPLEXITY 12.9673
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 3.83607 PERPLEXITY 14.2815

Model3 Viterbi Iteration : 2 took: 0 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 51.5309 #alsophisticatedcountcollection: 0 #hcsteps: 1.64501
#peggingImprovements: 0
A/D table contains 14601 parameters.
A/D table contains 15214 parameters.
p0_count is 58441.4 and p1 is 605.792; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.48567 PERPLEXITY 11.2019
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 3.58521 PERPLEXITY 12.002

Model3 Viterbi Iteration : 3 took: 1 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 51.4874 #alsophisticatedcountcollection: 7.82456 #hcsteps: 1.68796
#peggingImprovements: 0
D4 table contains 381234 parameters.
A/D table contains 14601 parameters.
A/D table contains 15079 parameters.
p0_count is 58831.5 and p1 is 410.755; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 3.40965 PERPLEXITY 10.6269
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 3.49223 PERPLEXITY 11.253

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 51.4621 #alsophisticatedcountcollection: 6.68586 #hcsteps: 1.64896
#peggingImprovements: 0
D4 table contains 381234 parameters.
A/D table contains 14601 parameters.
A/D table contains 15122 parameters.
p0_count is 58907 and p1 is 373.005; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.24606 PERPLEXITY 9.48772
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 3.31342 PERPLEXITY 9.94123

Model4 Viterbi Iteration : 5 took: 1 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 51.433 #alsophisticatedcountcollection: 6.05943 #hcsteps: 1.65333
#peggingImprovements: 0
D4 table contains 381234 parameters.
A/D table contains 14601 parameters.
A/D table contains 14896 parameters.
p0_count is 58921.9 and p1 is 365.574; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.16936 PERPLEXITY 8.99648
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 3.22832 PERPLEXITY 9.37179

Model4 Viterbi Iteration : 6 took: 1 seconds
H333444 Training Finished at: Sun Aug  4 23:30:22 2019


Entire Viterbi H333444 Training took: 4 seconds
==========================================================

Entire Training took: 6 seconds
Program Finished at: Sun Aug  4 23:30:22 2019

==========================================================
finished at နွေ ဩ 4 23:30:22 +0630 2019
